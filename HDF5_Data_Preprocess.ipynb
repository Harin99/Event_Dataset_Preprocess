{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNRbbyiv5K/8FMpz5Sniyh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harin99/Event_Dataset_Preprocess/blob/main/HDF5_Data_Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q4NJ9yDzq31w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os \n",
        "import sys\n",
        "import h5py \n",
        "import torch "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hdf5 File Read , Check "
      ],
      "metadata": {
        "id": "vcO3vsqRq6Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traverse_datasets(hdf_file):\n",
        "\n",
        "    def h5py_dataset_iterator(g, prefix=''):\n",
        "        for key in g.keys():\n",
        "            item = g[key]\n",
        "            #print(item) # <HDF5 gorup \"/davis\" ( 2 members )\n",
        "            path = f'{prefix}/{key}'\n",
        "            #print(path) # /davis\n",
        "            if isinstance(item, h5py.Dataset):\n",
        "                yield (path, item)\n",
        "            elif isinstance(item, h5py.Group): \n",
        "                yield from h5py_dataset_iterator(item, path)\n",
        "\n",
        "    for path, what in h5py_dataset_iterator(hdf_file):\n",
        "        yield path, what"
      ],
      "metadata": {
        "id": "3rc2rBbOq7Px"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/share/data/indoor_flying1/indoor_flying1_data.hdf5'\n",
        "gt_path = '/share/data/indoor_flying1/indoor_flying1_gt.hdf5'\n",
        "\n",
        "with h5py.File(data_path, 'r') as f:\n",
        "    for dset, _ in traverse_datasets(f):\n",
        "        print('Path:', dset) \n",
        "        print('Shape:', f[dset].shape)\n",
        "        print('Data type:', f[dset].dtype)\n",
        "        print('===========================')\n",
        "        #print('Data: ', f[dest][:])"
      ],
      "metadata": {
        "id": "346uWMpNq83N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hdf5 File Read & Data Extraction "
      ],
      "metadata": {
        "id": "-AYB3KtArzx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(data_path, 'r') as f:\n",
        "    for dset, _ in traverse_datasets(f):\n",
        "        # Path \n",
        "        print('Path: ', dset)\n",
        "        # All file data print \n",
        "        print('Data: ', f[dset][:])\n",
        "        break"
      ],
      "metadata": {
        "id": "YNVRGpJMq9m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwQKRpx5q-ms"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}